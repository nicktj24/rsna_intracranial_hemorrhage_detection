{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import glob, pylab, pandas as pd\nimport pydicom, numpy as np\nfrom os import listdir\nfrom os.path import isfile, join\nimport matplotlib.pylab as plt\nimport os\nimport seaborn as sns\nfrom imgaug import augmenters as iaa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras import layers\nfrom keras.applications import DenseNet121\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom keras.utils import to_categorical, Sequence\nimport cv2\nfrom keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images_dir = '../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images/'\n# train_images = [f for f in listdir(train_images_dir) if isfile(join(train_images_dir, f))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"     \n!ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(listdir('../input/rsna-intracranial-hemorrhage-detection'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images_dir = '../input/rsna-intracranial-hemorrhage-detection/stage_1_test_images/'\n# test_images = [ f for f in listdir(test_images_dir) if isfile(join(test_images_dir,f))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/rsna-intracranial-hemorrhage-detection/stage_1_train.csv')\ntrain['Subtype'] = train['ID'].str.rsplit('_', 1, expand=True)[1]\ntrain['ID'] = train['ID'].str.rsplit('_', 1, expand=True)[0]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.pivot_table(train, index=\"ID\", columns=\"Subtype\", values=\"Label\" )\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"test_data "},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/rsna-intracranial-hemorrhage-detection/stage_1_sample_submission.csv')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Subtype'] = test['ID'].str.rsplit('_', 1, expand=True)[1]\ntest['ID'] = test['ID'].str.rsplit('_', 1, expand=True)[0]\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.pivot_table(test, index=\"ID\", columns=\"Subtype\", values=\"Label\")\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Healthy Patient & Diagnosed Patients**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sick_patients = train[train[\"any\"]==1].index.values\n# # print(len(sick_patients))\n# # sick_patients = sick_patients[0:65536]\n# #######################\n# seed = 1\n\n# healthy_patients = train[ train[\"any\"] == 0 ].index.values\n# healthy_patients_selection = np.random.RandomState(seed).choice(\n#     healthy_patients, size=len(sick_patients), replace=False\n# )\n\n# selected_patients = list(set(healthy_patients_selection).union(set(sick_patients)))\n\n# updated_train = train.loc[selected_patients].copy()\n# len(healthy_patients_selection)\n# updated_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_seed = 1\ntrain_data, dev_data = train_test_split(train,\n                                        test_size=0.3,\n                                        stratify=train.values,\n                                        random_state=split_seed)\nprint(train_data.shape)\nprint(dev_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DataGenerator**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def correct_dcm(dcm):\n    x = dcm.pixel_array + 1000\n    px_mode = 4096\n    x[x>=px_mode] = x[x>=px_mode] - px_mode\n    dcm.PixelData = x.tobytes()\n    dcm.RescaleIntercept = -1000\n\ndef window_image(dcm):\n    window_center = 15\n    window_width = 400 \n    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n        correct_dcm(dcm)\n    \n    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n    img_min = window_center - window_width // 2\n    img_max = window_center + window_width // 2\n    img = np.clip(img, img_min, img_max)\n\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(Sequence):\n    def __init__(self, path, list_IDs, labels, batch_size, img_size, img_channel, num_classes, shuffle=True):\n        self.path = path\n        self.list_IDs = list_IDs\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_channel = img_channel\n        self.num_classes = num_classes\n        self.shuffle = shuffle\n        self.on_epoch_end()\n     \n    \n    def __len__(self):\n        return int(np.floor(len(self.list_IDs)/self.batch_size))\n    \n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y\n    \n#     def augment_img(self, image): \n#         augment_img = iaa.Sequential([\n#             iaa.Crop(keep_size=True, percent=(0.01, 0.05), sample_independently=False),\n#             iaa.Affine(rotate=(-10, 10)),\n#             iaa.Fliplr(0.5)])\n#         image_aug = augment_img.augment_image(image)\n#         return image_aug\n    \n    \n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n    \n    \n#     def rescale_pixelarray(self, dataset):\n#         image = dataset.pixel_array\n#         rescaled_image = image * dataset.RescaleSlope + dataset.RescaleIntercept\n#         rescaled_image[rescaled_image < -1024] = -1024\n#         return rescaled_image\n\n    \n    def __data_generation(self, list_IDs_temp):\n        X = np.empty((self.batch_size, self.img_size, self.img_size))\n        y = np.empty((self.batch_size, self.num_classes), dtype=np.float32)\n        for i, ID in enumerate(list_IDs_temp):\n            data_file = pydicom.dcmread(self.path+ ID +'.dcm')\n#             img = self.rescale_pixelarray(data_file)\n            img =  window_image(data_file)\n            img = cv2.resize(img, (self.img_size, self.img_size))\n#             img = cv2.resize(img, desired_size[:2], interpolation=cv2.INTER_LINEAR)\n#             img = self.augment_img(img)\n#             img = (img - np.min(img)) * (1 / (np.max(img) - np.min(img))) \n            X[i, ] = img\n            y[i, ] = self.labels.loc[ID]\n        X = np.repeat(X[..., np.newaxis], 1, -1)\n#         X = X.astype('float32')\n#         X -= X.mean(axis=0)\n#         std = X.std(axis=0)\n#         X /= X.std(axis=0)\n        return X, y\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test_data Generator**"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 8\nimg_size = 256\nimg_channel = 1\nnum_classes = 6\ntrain_generator = DataGenerator(train_images_dir, list(train_data.index), train_data,\n                                batch_size, img_size, img_channel, num_classes)\nval_generator = DataGenerator(train_images_dir, list(dev_data.index), dev_data,\n                                batch_size, img_size, img_channel, num_classes)\ntest_generator = DataGenerator(test_images_dir, list(test.index), test,\n                                batch_size, img_size, img_channel, num_classes)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Vgg16 Model Architecture**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\",input_shape=(256,256,1)))\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation=\"relu\",activity_regularizer=regularizers.l2(0.0001)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128, activation=\"relu\",activity_regularizer=regularizers.l2(0.0001)))\nmodel.add(Dropout(0.2))\n# model.add(Dense(128, input_dim=256,\n#                 kernel_regularizer=regularizers.l2(0.0001),\n#                 activity_regularizer=regularizers.l2(0.0001)))\n# model.add(Dense(num_classes, activation=\"softmax\"))\nmodel.add(Dense(num_classes, activation=\"sigmoid\"))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras import backend as K\nfrom keras.layers import Input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_tensor = Input(shape=(224, 224, 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = InceptionV3(weights='../input/inception-v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False,input_tensor=input_tensor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\npredictions = Dense(6, activation='sigmoid')(x)\nmodel = Model(inputs=base_model.input, outputs=predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(inputs=base_model.input, outputs=predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(base_model.input)\nprint(add_model(base_model.output))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\ndef weighted_log_loss(y_true, y_pred):\n    \"\"\"\n    Can be used as the loss function in model.compile()\n    ---------------------------------------------------\n    \"\"\"\n    \n    class_weights = np.array([0.25, 0.15, 0.15, 0.15, 0.15, 0.15])\n    \n    eps = K.epsilon()\n    \n    y_pred = K.clip(y_pred, eps, 1.0-eps)\n\n    out = -(         y_true  * K.log(      y_pred) * class_weights\n            + (1.0 - y_true) * K.log(1.0 - y_pred) * class_weights)\n    \n    return K.mean(out, axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Adam = keras.optimizers.Adam(learning_rate=0.00001)\nmodel.compile(loss=\"binary_crossentropy\",\n              optimizer=Adam, \n              metrics=[weighted_log_loss])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(generator=train_generator, validation_data=val_generator, epochs=1,max_queue_size=50,workers=50, use_multiprocessing=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.models import load_model\n# import h5py\n# # model.save('my_model.h5')\n# model = load_model('my_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 5\ntest_generator = DataGenerator(test_images_dir, list(test.index), test,\n                                batch_size, img_size, img_channel, num_classes) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict_generator (test_generator, steps=15709, workers=10, use_multiprocessing=False, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/rsna-intracranial-hemorrhage-detection/stage_1_sample_submission.csv')\ntest_df = submission.ID.str.rsplit(\"_\", 1, expand=True)\ntest_df = test_df.rename({0:\"ID\", 1:'subtype'}, axis=1)\ntest_df.loc[:, \"Label\"] = 0\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def turn_pred_to_dataframe(test_df, predictions):\n    df = pd.DataFrame(predictions, columns=test_df.columns, index=test_df.index)\n    df = df.stack().reset_index()\n    df.loc[:, 'ID'] = df.ID.str.cat(df.Subtype, sep=\"_\")\n    df = df.drop([\"Subtype\"], axis=1)\n    df = df.rename({0:'Label'}, axis =1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testpred_df = turn_pred_to_dataframe(test, predictions )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testpred_df.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testpred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# os.chdir(r'/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testpred_df.to_csv(r'df_name.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink(r'df_name.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}